class EnhancedResumeParser:
    def __init__(self):
        # Initialize NLP components
        self.nlp = spacy.load("en_core_web_lg")  # Requires: pip install spacy && python -m spacy download en_core_web_lg
        self.stop_words = set(stopwords.words('english'))
        
        # Load skill taxonomies
        self.skills_dict = self._load_skills_dictionary()
        
    def _load_skills_dictionary(self):
        """Load categorized skills dictionary"""
        return {
            "programming_languages": [
                "Python", "Java", "JavaScript", "TypeScript", "C++", "C#", "Ruby", "Go", "Swift", 
                "Kotlin", "PHP", "Rust", "Scala", "Perl", "R", "MATLAB", "Bash", "PowerShell"
            ],
            "web_technologies": [
                "HTML", "CSS", "SASS", "LESS", "Bootstrap", "Tailwind CSS", "jQuery", "JSON", "XML",
                "RESTful API", "GraphQL", "SOAP", "WebSockets", "PWA", "Web Components"
            ],
            "frameworks": [
                "React", "Angular", "Vue.js", "Next.js", "Django", "Flask", "Spring", "Express.js",
                "Node.js", "Laravel", "ASP.NET", "Ruby on Rails", "Symfony", "FastAPI", "Svelte"
            ],
            "databases": [
                "MySQL", "PostgreSQL", "MongoDB", "SQLite", "Oracle", "SQL Server", "Redis", "Cassandra",
                "DynamoDB", "Firebase", "Neo4j", "Elasticsearch", "MariaDB", "CouchDB"
            ],
            "cloud_services": [
                "AWS", "Azure", "Google Cloud", "Heroku", "DigitalOcean", "Netlify", "Vercel",
                "Firebase", "S3", "EC2", "Lambda", "CloudFront", "Azure Functions", "GCP Functions"
            ],
            "devops": [
                "Docker", "Kubernetes", "Jenkins", "CI/CD", "GitHub Actions", "Travis CI", "CircleCI",
                "Terraform", "Ansible", "Chef", "Puppet", "ELK Stack", "Prometheus", "Grafana"
            ],
            "data_science": [
                "Machine Learning", "Deep Learning", "Data Analysis", "Data Mining", "NLP", "Computer Vision",
                "TensorFlow", "PyTorch", "scikit-learn", "Pandas", "NumPy", "SciPy", "Matplotlib",
                "Seaborn", "Tableau", "Power BI", "NLTK", "Keras", "Hadoop", "Spark"
            ]
        }

    def extract_education(self, text):
        """Enhanced education extraction with comprehensive patterns"""
        education = []
        
        # More comprehensive education patterns
        degree_patterns = [
            # Common degree patterns
            r'(?i)(?:Bachelor|BS|BSc|BA|B\.A\.|B\.S\.|B\.Tech|B\.E\.)\s*(?:of|in|')?\s*(?:Science|Arts|Engineering|Technology|Business|Finance|Accounting|Economics|Mathematics|Computer Science|Computer Engineering|Information Technology|Information Systems)',
            r'(?i)(?:Master|MS|MSc|MA|M\.A\.|M\.S\.|M\.Tech|M\.E\.)\s*(?:of|in|')?\s*(?:Science|Arts|Engineering|Technology|Business|Finance|MBA|Accounting|Economics|Mathematics|Computer Science|Computer Engineering|Information Technology|Information Systems)',
            r'(?i)(?:PhD|Ph\.D\.|Doctorate|Doctoral)\s*(?:of|in|')?\s*(?:Science|Arts|Engineering|Technology|Business|Finance|Accounting|Economics|Mathematics|Computer Science|Computer Engineering|Information Technology|Philosophy)',
            r'(?i)(?:Associate|Diploma|Certificate)\s*(?:of|in|')?\s*(?:Science|Arts|Engineering|Technology|Business|Applied Science)',
            
            # International degree formats
            r'(?i)(?:B\.Tech|B\.E\.|BTech|BE)\s*(?:in)?\s*(?:[\w\s]+)',
            r'(?i)(?:M\.Tech|M\.E\.|MTech|ME)\s*(?:in)?\s*(?:[\w\s]+)',
        ]
        
        institution_patterns = [
            # Common institution patterns
            r'(?i)(?:University|College|Institute|School) of [\w\s]+',
            r'(?i)(?:[\w\s]+) (?:University|College|Institute|School|Academy|Polytechnic)',
            r'(?i)(?:IIT|NIT|IIIT|BITS) (?:[\w\s]+)',
            
            # International institution patterns
            r'(?i)(?:Technical University of [\w\s]+)',
            r'(?i)(?:[\w\s]+) Institute of Technology',
        ]
        
        graduation_patterns = [
            # Graduation date patterns
            r'(?i)(?:Graduated|Graduation|Completed|Expected|Exp\.?|Anticipated): (?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[\.,]? \d{4}',
            r'(?i)(?:Graduated|Graduation|Completed|Expected|Exp\.?|Anticipated): \d{4}',
            r'(?i)\d{4} - (?:Present|Current|Now|\d{4})',
            r'(?i)(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[\.,]? \d{4} - (?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[\.,]? \d{4}',
            r'(?i)(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[\.,]? \d{4} - (?:Present|Current|Now)',
        ]
        
        grade_patterns = [
            # GPA patterns
            r'(?i)(?:GPA|CGPA)[\s:]*(?: of)? \d+\.\d+(?:/\d+\.\d+)?',
            r'(?i)(?:First Class|Second Class|Distinction|Merit|Honors|Honours|cum laude|magna cum laude|summa cum laude)',
        ]
        
        # Combine all patterns
        all_patterns = degree_patterns + institution_patterns + graduation_patterns + grade_patterns
        
        # First try to identify sections
        education_section = self._extract_section(text, ["Education", "Academic Background", "Academic Qualifications"])
        
        # If education section found, apply patterns to it
        if education_section:
            text_to_search = education_section
        else:
            text_to_search = text
            
        # Extract using patterns
        for pattern in all_patterns:
            matches = re.findall(pattern, text_to_search)
            education.extend(matches)
            
        # Deduplicate
        return list(set(education))
    
    def extract_skills(self, text):
        """Enhanced skill extraction with categorization and context awareness"""
        skills_found = set()
        
        # First try to identify skills section
        skills_section = self._extract_section(text, ["Skills", "Technical Skills", "Core Skills", "Key Skills"])
        
        # If skill section found, focus on it
        if skills_section:
            text_to_search_primary = skills_section
            text_to_search_secondary = text
        else:
            text_to_search_primary = text
            text_to_search_secondary = ""
            
        # Lowercase for easier matching
        text_lower = text_to_search_primary.lower()
        
        # 1. Extract from predefined taxonomy
        for category, skills_list in self.skills_dict.items():
            for skill in skills_list:
                skill_lower = skill.lower()
                # Try exact match with word boundaries
                pattern = r'\b' + re.escape(skill_lower) + r'\b'
                if re.search(pattern, text_lower):
                    skills_found.add((skill, 1.0, category))  # Full confidence for exact match
                else:
                    # Try partial matches for multi-word skills
                    if ' ' in skill_lower:
                        words = skill_lower.split()
                        if all(re.search(r'\b' + re.escape(word) + r'\b', text_lower) for word in words):
                            skills_found.add((skill, 0.8, category))  # Good confidence for all words separately
                            
        # 2. Look for skill level indicators
        skill_levels = ["beginner", "intermediate", "advanced", "expert", "proficient", "familiar", "experienced"]
        
        for skill, confidence, category in list(skills_found):
            for level in skill_levels:
                pattern = fr'\b{level}\b.{{0,30}}\b{re.escape(skill.lower())}\b|\b{re.escape(skill.lower())}\b.{{0,30}}\b{level}\b'
                if re.search(pattern, text_lower):
                    # Update with level information
                    skills_found.remove((skill, confidence, category))
                    skills_found.add((f"{skill} ({level.title()})", confidence, category))
                    
        # 3. Use NLP for entity recognition if spaCy is available
        try:
            # Process with spaCy
            doc = self.nlp(text_to_search_primary)
            
            # Extract entities that might be skills
            for ent in doc.ents:
                if ent.label_ in ["PRODUCT", "ORG", "GPE", "LANGUAGE"]:
                    candidate = ent.text.strip()
                    # Check if not already found and reasonable length
                    if len(candidate) > 2 and not any(candidate.lower() == skill.lower() for skill, _, _ in skills_found):
                        skills_found.add((candidate, 0.6, "nlp_detected"))
        except:
            # If spaCy fails or not installed, continue without it
            pass
            
        # 4. Search for common skills in secondary text if needed
        if text_to_search_secondary:
            for category, skills_list in self.skills_dict.items():
                for skill in skills_list:
                    if not any(skill.lower() == s.lower() for s, _, _ in skills_found):
                        if re.search(r'\b' + re.escape(skill.lower()) + r'\b', text_to_search_secondary.lower()):
                            skills_found.add((skill, 0.7, category))  # Lower confidence for secondary text
                            
        # Format results - convert to list of skills with confidence â‰¥ 0.7
        final_skills = []
        for skill, confidence, category in skills_found:
            if confidence >= 0.7:
                final_skills.append(skill)
                
        return final_skills
    
    def extract_experience(self, text):
        """Enhanced experience extraction with company, duration, and responsibilities"""
        experience_entries = []
        
        # Try to identify experience section
        experience_section = self._extract_section(
            text, 
            ["Experience", "Work Experience", "Professional Experience", "Employment History", "Work History"]
        )
        
        # If experience section found, use it
        if experience_section:
            text_to_search = experience_section
        else:
            text_to_search = text
            
        # Comprehensive job title patterns
        job_titles = [
            # Technology roles
            r'(?i)(?:Senior|Junior|Lead|Principal|Staff|Chief)?\s*(?:Software|Systems|Data|Full Stack|Front[- ]?End|Back[- ]?End|Web|Mobile|Cloud|DevOps|QA|Test|UI/UX|UX|UI)\s*(?:Engineer|Developer|Architect|Analyst|Scientist|Designer)',
            
            # Management roles
            r'(?i)(?:Project|Product|Program|Technical|Engineering|Development|Operations)\s*(?:Manager|Lead|Director)',
            
            # Executive roles
            r'(?i)(?:Director|VP|Vice President|CTO|CEO|CIO|COO|CISO|CSO|CMO|CFO)\b',
            
            # Other technical roles
            r'(?i)(?:Database|Network|Security|Systems|Infrastructure|Cloud|AI/ML)\s*(?:Administrator|Engineer|Specialist|Consultant)',
            
            # Non-technical roles
            r'(?i)(?:Marketing|Sales|HR|Finance|Operations|Business)\s*(?:Specialist|Analyst|Coordinator|Associate|Manager|Executive)',
            
            # Entry level/internship
            r'(?i)(?:Intern|Internship|Trainee|Graduate|Associate|Junior)\s*(?:in|at|with)?\s*(?:[\w\s]+)?',
        ]
        
        # Company name patterns
        company_patterns = [
            r'(?i)(?:at|with|for)\s+([\w\s&\-\.]+?)(?:,|\.|in|\(|\)|\d|from|\s{2})',
            r'(?i)([\w\s&\-\.]+?)(?:Company|Corp\.?|Corporation|Inc\.?|Ltd\.?|LLC|GmbH|Limited)',
        ]
        
        # Duration patterns
        duration_patterns = [
            r'(?i)(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*[\.,]?\s+\d{4}\s*(?:-|â€“|to)\s*(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*[\.,]?\s+\d{4}',
            r'(?i)(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*[\.,]?\s+\d{4}\s*(?:-|â€“|to)\s*(?:Present|Current|Now)',
            r'(?i)\d{4}\s*(?:-|â€“|to)\s*\d{4}',
            r'(?i)\d{4}\s*(?:-|â€“|to)\s*(?:Present|Current|Now)',
        ]
        
        # Try to find role-company-duration patterns
        # Use regex to find job blocks with title, company and duration
        job_blocks = re.findall(
            r'((?:[A-Z][a-z]+\s*){1,5}(?:Engineer|Developer|Manager|Director|Analyst|Designer|Lead|Architect|Specialist|Consultant|Associate|Intern))\s*(?:at|with|for|-)?\s*((?:[A-Z][a-zA-Z0-9]+\s*){1,3}(?:Inc\.?|LLC|Corp\.?|Ltd\.?|GmbH|Limited)?)\s*(?:\(|\||\n)?\s*((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)?\.?\s*\d{4}\s*(?:-|â€“|to)\s*(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)?\.?\s*\d{4}|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)?\.?\s*\d{4}\s*(?:-|â€“|to)\s*(?:Present|Current|Now))',
            text_to_search
        )
        
        # Process job blocks found
        for title, company, duration in job_blocks:
            experience_entries.append({
                'title': title.strip(),
                'company': company.strip(),
                'duration': duration.strip()
            })
        
        # If no structured blocks found, try individual patterns
        if not experience_entries:
            # First get job titles
            found_titles = []
            for pattern in job_titles:
                matches = re.findall(pattern, text_to_search)
                found_titles.extend(matches)
                
            # Try to find companies 
            found_companies = []
            for pattern in company_patterns:
                matches = re.findall(pattern, text_to_search)
                found_companies.extend(matches)
                
            # Get durations
            found_durations = []
            for pattern in duration_patterns:
                matches = re.findall(pattern, text_to_search)
                found_durations.extend(matches)
                
            # Try to match them by proximity
            for title in found_titles[:3]:  # Limit to top 3 to avoid too many entries
                experience_entry = {'title': title}
                
                # Find closest company and duration
                title_pos = text_to_search.find(title)
                
                closest_company = None
                min_company_dist = float('inf')
                for company in found_companies:
                    company_pos = text_to_search.find(company)
                    dist = abs(title_pos - company_pos)
                    if dist < min_company_dist and dist < 200:  # Within reasonable distance
                        min_company_dist = dist
                        closest_company = company
                
                closest_duration = None
                min_duration_dist = float('inf')
                for duration in found_durations:
                    duration_pos = text_to_search.find(duration)
                    dist = abs(title_pos - duration_pos)
                    if dist < min_duration_dist and dist < 200:  # Within reasonable distance
                        min_duration_dist = dist
                        closest_duration = duration
                
                if closest_company:
                    experience_entry['company'] = closest_company
                if closest_duration:
                    experience_entry['duration'] = closest_duration
                    
                if len(experience_entry) > 1:  # Must have at least title and one other field
                    experience_entries.append(experience_entry)
        
        # Format the results for compatibility with the current implementation
        formatted_experience = []
        for entry in experience_entries:
            formatted_entry = entry.get('title', '')
            if 'company' in entry:
                formatted_entry += f" at {entry['company']}"
            if 'duration' in entry:
                formatted_entry += f" ({entry['duration']})"
            formatted_experience.append(formatted_entry)
            
        return formatted_experience
    
    def _extract_section(self, text, section_headers):
        """Extracts a section of the resume based on headers"""
        for header in section_headers:
            # Try different header formats
            patterns = [
                rf'(?i)(?:^|\n){re.escape(header)}(?::|\s*\n)',
                rf'(?i)(?:^|\n)(?:\d+\.|â€¢|\*)\s*{re.escape(header)}(?::|\s*\n)',
                rf'(?i)(?:^|\n)(?:<[^>]*>)?\s*{re.escape(header)}\s*(?:</[^>]*>)?(?::|\s*\n)'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, text)
                if match:
                    # Find start position
                    start_pos = match.end()
                    
                    # Find the next section header
                    next_header_pattern = r'(?i)(?:^|\n)(?:[A-Z][A-Za-z\s]+:|\d+\.\s*[A-Z][A-Za-z\s]+|\n\s*[A-Z][A-Za-z\s]+\s*\n)'
                    next_match = re.search(next_header_pattern, text[start_pos:])
                    
                    # Extract section
                    if next_match:
                        return text[start_pos:start_pos + next_match.start()].strip()
                    else:
                        return text[start_pos:].strip()
                        
        # No section found
        return ""

    def calculate_enhanced_ats_score(self, parsed_data, job_description=None):
        """Calculate a more nuanced ATS score based on parsed data"""
        score = 0
        max_score = 100
        
        # Initialize weighted categories
        category_weights = {
            'skills': 40,            # 40% of total score
            'education': 25,         # 25% of total score
            'experience': 30,        # 30% of total score
            'formatting': 5          # 5% of total score
        }
        
        # SKILLS SCORE - up to 40 points
        # Count skills by category to add nuance
        skills_score = 0
        skill_weights = {
            'programming_languages': 1.5,
            'web_technologies': 1.2,
            'frameworks': 1.3,
            'databases': 1.0,
            'cloud_services': 1.4,
            'devops': 1.2,
            'data_science': 1.3
        }
        
        # Basic skills count (for backward compatibility)
        skills_count = len(parsed_data["skills"])
        skills_score = min(skills_count * 2.0, category_weights['skills'] * 0.8)  # Cap at 80% of skill points
        
        # Add bonus for diverse skills
        skill_categories_found = set()
        for skill in parsed_data["skills"]:
            for category, skills in self.skills_dict.items():
                if any(s.lower() == skill.lower() for s in skills):
                    skill_categories_found.add(category)
                    break
        
        # Add diversity bonus - up to 20% of skill points
        diversity_bonus = min(len(skill_categories_found) * 1.5, category_weights['skills'] * 0.2)
        skills_score += diversity_bonus
        
        # Cap skills score
        skills_score = min(skills_score, category_weights['skills'])
        score += skills_score
        
        # EDUCATION SCORE - up to 25 points
        education_score = 0
        
        # Check for specific degree levels
        degree_points = {
            'bachelor': 10,
            'master': 15,
            'phd': 20,
            'associate': 5,
            'certificate': 2
        }
        
        # Create degree patterns to check
        degree_patterns = {
            'bachelor': r'(?i)bachelor|bs|bsc|b\.s\.|ba|b\.a\.|b\.tech|btech',
            'master': r'(?i)master|ms|msc|m\.s\.|ma|m\.a\.|m\.tech|mtech|mba',
            'phd': r'(?i)phd|ph\.d|doctor|doctorate',
            'associate': r'(?i)associate|diploma',
            'certificate': r'(?i)certificate|certification'
        }
        
        highest_degree = None
        highest_points = 0
        
        for edu in parsed_data["education"]:
            for degree, pattern in degree_patterns.items():
                if re.search(pattern, edu):
                    points = degree_points[degree]
                    if points > highest_points:
                        highest_points = points
                        highest_degree = degree
        
        education_score = highest_points if highest_degree else 0
        
        # Add points for relevant field of study
        relevant_fields = ["computer science", "information technology", "software engineering", 
                           "data science", "computer engineering", "information systems"]
        
        for edu in parsed_data["education"]:
            if any(field in edu.lower() for field in relevant_fields):
                education_score += 5
                break
                
        # Cap education score
        education_score = min(education_score, category_weights['education'])
        score += education_score
        
        # EXPERIENCE SCORE - up to 30 points
        experience_score = 0
        
        # Extract durations when available
        total_years = 0
        for exp in parsed_data["experience"]:
            # Try to extract years of experience
            years_match = re.search(r'(\d+)(?:\+)?\s*years?', exp, re.IGNORECASE)
            if years_match:
                total_years += int(years_match.group(1))
                continue
                
            # Try to extract duration from date ranges
            duration_match = re.search(r'(\d{4})\s*(?:-|â€“|to)\s*(\d{4}|Present|Current|Now)', exp, re.IGNORECASE)
            if duration_match:
                start_year = int(duration_match.group(1))
                if duration_match.group(2).isdigit():
                    end_year = int(duration_match.group(2))
                else:
                    # If "Present", use current year
                    import datetime
                    end_year = datetime.datetime.now().year
                total_years += (end_year - start_year)
        
        # Score based on years of experience and job titles
        if total_years > 0:
            # 3 points per year, max 15 points
            experience_score += min(total_years * 3, 15)
        else:
            # Fallback to just counting experiences
            experience_count = len(parsed_data["experience"])
            experience_score += min(experience_count * 5, 15)
        
        # Look for senior roles
        senior_patterns = r'(?i)senior|lead|principal|manager|director|chief|head'
        for exp in parsed_data["experience"]:
            if re.search(senior_patterns, exp):
                experience_score += 5
                break
                
        # Look for relevant industry experience
        relevant_industries = ["tech", "software", "it", "consulting", "engineering"]
        for exp in parsed_data["experience"]:
            if any(industry in exp.lower() for industry in relevant_industries):
                experience_score += 5
                break
                
        # Cap experience score
        experience_score = min(experience_score, category_weights['experience'])
        score += experience_score
        
        # FORMATTING SCORE - up to 5 points
        # Assume well-formatted if parsing was successful
        score += category_weights['formatting']
        
        # Match with job description if provided
        if job_description:
            job_matching_score = self._calculate_job_matching(parsed_data, job_description)
            # Apply job matching as a multiplier (0.8-1.2)
            score = score * (0.8 + (job_matching_score * 0.4))
        
        # Cap at 100
        score = min(score, 100)
        
        return score
        
    def _calculate_job_matching(self, parsed_data, job_description):
        """Calculate how well the resume matches the job description"""
        # Tokenize job description
        job_tokens = word_tokenize(job_description.lower())
        job_tokens = [token for token in job_tokens if token.isalnum() and token not in self.stop_words]
        
        # Get resume skills
        resume_skills = [skill.lower() for skill in parsed_data["skills"]]
        
        # Count skill matches
        matched_skills = 0
        for skill in resume_skills:
            skill_words = skill.split()
            if len(skill_words) > 1:
                # For multi-word skills, check if all words appear close to each other
                if all(word in job_tokens for word in skill_words):
                    matched_skills += 1
            else:
                # For single-word skills, check for exact match
                if skill in job_tokens:
                    matched_skills += 1
        
        # Calculate matching ratio (0.0 to 1.0)
        matching_ratio = matched_skills / max(1, len(resume_skills))
        
        return matching_ratio